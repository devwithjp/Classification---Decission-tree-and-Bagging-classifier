{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKwEuKXKbg5e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying link where the csv dataset is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9anf1rAEb1qA"
   },
   "outputs": [],
   "source": [
    "url='https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the csv file using the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sVWaZ3rob69W"
   },
   "outputs": [],
   "source": [
    "Suzy=pd.read_csv(url,header=None,index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head of the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7XEnl7xpb9s0",
    "outputId": "e1da132e-ecf0-4b1e-8474-beae620468f6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972861</td>\n",
       "      <td>0.653855</td>\n",
       "      <td>1.176225</td>\n",
       "      <td>1.157156</td>\n",
       "      <td>-1.739873</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>-0.252552</td>\n",
       "      <td>1.921887</td>\n",
       "      <td>0.889637</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>1.145621</td>\n",
       "      <td>1.932632</td>\n",
       "      <td>0.994464</td>\n",
       "      <td>1.367815</td>\n",
       "      <td>0.040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "      <td>1.321893</td>\n",
       "      <td>0.377584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.526283</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>2.024308</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "      <td>1.135454</td>\n",
       "      <td>0.180910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>1.551914</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "      <td>1.492257</td>\n",
       "      <td>0.090719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>1.179295</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "      <td>1.154854</td>\n",
       "      <td>0.094859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  0.0  0.972861  0.653855  1.176225  1.157156 -1.739873 -0.874309  0.567765   \n",
       "1  1.0  1.667973  0.064191 -1.225171  0.506102 -0.338939  1.672543  3.475464   \n",
       "2  1.0  0.444840 -0.134298 -0.709972  0.451719 -1.613871 -0.768661  1.219918   \n",
       "3  1.0  0.381256 -0.976145  0.693152  0.448959  0.891753 -0.677328  2.033060   \n",
       "4  1.0  1.309996 -0.690089 -0.676259  1.589283 -0.693326  0.622907  1.087562   \n",
       "\n",
       "         8         9         10        11        12        13        14  \\\n",
       "0 -0.175000  0.810061 -0.252552  1.921887  0.889637  0.410772  1.145621   \n",
       "1 -1.219136  0.012955  3.775174  1.045977  0.568051  0.481928  0.000000   \n",
       "2  0.504026  1.831248 -0.431385  0.526283  0.941514  1.587535  2.024308   \n",
       "3  1.533041  3.046260 -1.005285  0.569386  1.015211  1.582217  1.551914   \n",
       "4 -0.381742  0.589204  1.365479  1.179295  0.968218  0.728563  0.000000   \n",
       "\n",
       "         15        16        17        18  \n",
       "0  1.932632  0.994464  1.367815  0.040714  \n",
       "1  0.448410  0.205356  1.321893  0.377584  \n",
       "2  0.603498  1.562374  1.135454  0.180910  \n",
       "3  0.761215  1.715464  1.492257  0.090719  \n",
       "4  1.083158  0.043429  1.154854  0.094859  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suzy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays top few rows of the data frame, providing us with an idea of the values present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Dropping columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y54dcYR3cC51"
   },
   "outputs": [],
   "source": [
    "Features=Suzy.drop(columns=[0,11,14,17,18],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the part that makes the last section different from this one. Here only 4 out of the 8 attributes/features are selected and the rest are dropped along with labels to achieve the required features dataframe.\n",
    "- Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "k8EBl4H6cSSE",
    "outputId": "b8dc73ba-0b03-41ad-f0d0-21aebcdcecff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.972861</td>\n",
       "      <td>0.653855</td>\n",
       "      <td>1.176225</td>\n",
       "      <td>1.157156</td>\n",
       "      <td>-1.739873</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>-0.252552</td>\n",
       "      <td>0.889637</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>1.932632</td>\n",
       "      <td>0.994464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         1         2         3         4         5         6         7   \\\n",
       "0  0.972861  0.653855  1.176225  1.157156 -1.739873 -0.874309  0.567765   \n",
       "1  1.667973  0.064191 -1.225171  0.506102 -0.338939  1.672543  3.475464   \n",
       "2  0.444840 -0.134298 -0.709972  0.451719 -1.613871 -0.768661  1.219918   \n",
       "3  0.381256 -0.976145  0.693152  0.448959  0.891753 -0.677328  2.033060   \n",
       "4  1.309996 -0.690089 -0.676259  1.589283 -0.693326  0.622907  1.087562   \n",
       "\n",
       "         8         9         10        12        13        15        16  \n",
       "0 -0.175000  0.810061 -0.252552  0.889637  0.410772  1.932632  0.994464  \n",
       "1 -1.219136  0.012955  3.775174  0.568051  0.481928  0.448410  0.205356  \n",
       "2  0.504026  1.831248 -0.431385  0.941514  1.587535  0.603498  1.562374  \n",
       "3  1.533041  3.046260 -1.005285  1.015211  1.582217  0.761215  1.715464  \n",
       "4 -0.381742  0.589204  1.365479  0.968218  0.728563  1.083158  0.043429  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First few rows are displayed after dropping the first column and the unnecessay features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFM5PF2scsBj"
   },
   "outputs": [],
   "source": [
    "Labels=Suzy[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Labels dataframe from the 0th column of the dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying top few rows of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "-Fndr6eqcx9X",
    "outputId": "c0039a19-d4fe-48e2-c3d5-59b7e9691e9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that it is a binary classification problem, i.e the label has two values 1 or 0. Thus need to predict if its 1 or 0 based on the 14 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes of the dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "38cwSVarcziS",
    "outputId": "2acec898-108c-45c0-d685-0bf6da828520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000000, 14) (5000000,)\n"
     ]
    }
   ],
   "source": [
    "print(Features.shape , Labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 14 columns(4 features are dropped) and 5000000 rows in the Features database.\n",
    "And Labels have 1 column and 5000000 rows. Makes sense as there are 18 attributes and 5000000 data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary modules from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUNRJ5q4c-3B"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold,cross_validate\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn is a  library that features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means etc. Here, decision tree classification, model selection and metrics algorithms are imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jZiz2JxdHts"
   },
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(Features,Labels,test_size=0.3,random_state=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the above cell, the data is split into two subsets one for training and the other for testing.\n",
    "The first two parameters to the function are the dataframes which is required to be split. test_size determines the percentage of data used for testing.Here 30 percent of data is kept for testing and the rest 70% is used for training the classifier.\n",
    "The choice of test_data is based on two factors, if it is large enough to yield statistically meaningful results and if it\n",
    "is representative of the data set as a whole(Not to pick a test set with different characteristics than the training set.)\n",
    "\n",
    "The reason testing and training isn't performed on the same data is to test the generalisation of  the model. The whole purpose of training a model is to use it to infer different data that is given to it. To test on the same data that is used for training is analogous to a mathematics professor asking the same questions he taught the concepts with in the test(overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions after splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j9_tQF36dKes",
    "outputId": "8ee4b402-ccfd-4ff9-d1d4-26a25264dd76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500000, 14) (1500000, 14) (3500000,) (1500000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Training Features 3500000 x 14\n",
    "- Testing Features 1500000 x 14\n",
    "- Training Labels 3500000 x 1\n",
    "- Testing Labels 1500000 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0zHzqsv_dMwZ",
    "outputId": "84ef0afd-3449-4f1e-c31b-60ed6f2bdfe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "for dataset in [y_train,y_test]:\n",
    "    print(round(len(dataset) / len(Labels), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training percentage - 70\n",
    "- Testing perscentage - 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gUMTFPIYdOL6"
   },
   "outputs": [],
   "source": [
    "Suzy_DT = DecisionTreeClassifier(criterion=\"gini\", random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A decision tree classifier is used as the classification algorithm. It uses a decision tree to go from observations about an item to conclusions about the item's target value. It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. \n",
    "criterion is the function to measure the quality of a split.Here criteria is “gini” for the Gini impurity.\n",
    "random_state is the seed used by the random number generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "oEwb9xgndQCv",
    "outputId": "cfd956ee-f53c-42ec-d404-ff71b35fe260"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suzy_DT.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell is responsible for training the model. It takes in Features for training(x_train) and Labels for training (y_training) and fits them. Model fitting is essentially training and it helps provide a measure of how well a machine learning model generalizes to similar data to that on which it was fitted. A model that is well-fitted produces more accurate outcomes. A model that is overfitted matches the data too closely. A model that is underfitted doesn’t match closely enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KAbNaPDMdSOb"
   },
   "outputs": [],
   "source": [
    "y_pred=Suzy_DT.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model(function) is learnt and it can be used to predict the label of new data. This is essentially what the above cell is performing. It's taking in Features test( x_test) and predicting what it's corresponding y value must be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YL8B-wlFdZaL",
    "outputId": "2279af59-8cbf-4f74-94b3-84e38c572ca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7157366666666667"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Suzy_DT.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell computes the accuracy of the classifier.\n",
    "which is 71.57.. percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MulcLHmvdbE8"
   },
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "def tn(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions help place the scores in a confusion matrix and computes true and false, positive and negatives.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J72YufYrdcmK"
   },
   "outputs": [],
   "source": [
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n",
    "           'fp': make_scorer(fp), 'fn': make_scorer(fn),\n",
    "           'ac' : make_scorer(accuracy_score),\n",
    "           're' : make_scorer(recall_score),\n",
    "           'pr' : make_scorer(precision_score),\n",
    "           'f1' : make_scorer(f1_score),\n",
    "           'auc' : make_scorer(roc_auc_score),\n",
    "          } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary 'scoring' is used to address each of the evaluation metrics. make_scorer creates a scorer from a performance metric or loss function.\n",
    "- A true positive is an outcome where the model correctly predicts the positive class. A true negative is an outcome where the model correctly predicts the negative class.A false positive is an outcome where the model incorrectly predicts the positive class. And a false negative is an outcome where the model incorrectly predicts the negative class.\n",
    "- accuracy_score computes subset accuracy. \n",
    "Accuracy\n",
    "- Recall = tp / (tp + fn).It calculates how many of the actual positives our model capture through labeling it as Positive (True Positive).  If a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.\n",
    "- Precision = tp / (tp + fp) . Precision is a good measure to determine, when the costs of False Positive is high. For instance, email spam detection. In email spam detection, a false positive means that an email that is non-spam (actual negative) has been identified as spam (predicted spam). The email user might lose important emails if the precision is not high for the spam detection model.\n",
    "- f1_score=  = 2 * (precision * recall) / (precision + recall).  It can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0\n",
    "- roc_auc_score- An evaluation metric that considers all possible classification thresholds.The Area Under the ROC curve is the probability that a classifier will be more confident that a randomly chosen positive example is actually positive than that a randomly chosen negative example is positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-9txCNKdd7W"
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(Suzy_DT, x_train, y_train, scoring=scoring, cv=StratifiedKFold(n_splits=nfolds, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe above cell performs cross validation using the stratified kfold cross validation method. Validation is the process of making sure that the model generalizes well. Generalization is when model is built using one set of data and it performs well on a completely different set of data. \n",
    "- K-Fold Cross Validation- In this the data is split into K buckets called folds.K-fold validation evaluates the data across the entire training set, but it does so by dividing the training set into K folds — or subsections — (where K is a positive integer) and then training the model K times, each time leaving a different fold out of the training data and using it instead as a validation set.At the end, the performance metric (e.g. accuracy, ROC, etc.) is averaged across all K tests. Once the best parameter combination has been found, the model is retrained on the full data. In the startified k folds the validation returns stratified folds. The folds are made by preserving the percentage of samples for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying evaluation metrics results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "lfh7g6Oqdfrs",
    "outputId": "65058a28-425a-4fd3-a7d2-f4755dda8dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation scores (nfolds = 10):\n",
      "tp:  [111083 111168 111109 111204 111167 110785 111020 111337 111135 111260] ; mean: 111126.8\n",
      "fn:  [49032 48947 49006 48911 48947 49329 49094 48777 48979 48854] ; mean: 48987.6\n",
      "fp:  [51161 50954 50561 50655 50627 50897 50865 51059 50649 51027] ; mean: 50845.5\n",
      "tn:  [138725 138932 139325 139231 139259 138989 139020 138826 139236 138858] ; mean: 139040.1\n",
      "ac:  [0.7137351  0.71456939 0.71552367 0.71552653 0.71550286 0.71364\n",
      " 0.71440204 0.71475347 0.71534776 0.7146249 ] ; mean: 0.7147625713638519\n",
      "re:  [0.6937701  0.69430097 0.69393249 0.69452581 0.69429906 0.69191326\n",
      " 0.69338097 0.69536081 0.6940992  0.6948799 ] ; mean: 0.6940462567112372\n",
      "pr:  [0.68466631 0.68570583 0.68725799 0.68704243 0.68708976 0.68520305\n",
      " 0.68579547 0.68558955 0.68693443 0.68557555] ; mean: 0.6860860365206534\n",
      "f1:  [0.68918814 0.68997663 0.69057911 0.69076385 0.6906756  0.68854181\n",
      " 0.68956736 0.69044061 0.69049823 0.69019637] ; mean: 0.690042771112482\n",
      "auc:  [0.71217001 0.71298051 0.7138311  0.71388024 0.7138406  0.71193675\n",
      " 0.71275415 0.71323324 0.71368204 0.71307705] ; mean: 0.7131385680857917\n"
     ]
    }
   ],
   "source": [
    "print('Cross Validation scores (nfolds = %d):'% nfolds)\n",
    "print('tp: ', cv_results['test_tp'], '; mean:', cv_results['test_tp'].mean())\n",
    "print('fn: ', cv_results['test_fn'], '; mean:', cv_results['test_fn'].mean())\n",
    "print('fp: ', cv_results['test_fp'], '; mean:', cv_results['test_fp'].mean())\n",
    "print('tn: ', cv_results['test_tn'], '; mean:', cv_results['test_tn'].mean())\n",
    "print('ac: ', cv_results['test_ac'], '; mean:', cv_results['test_ac'].mean())\n",
    "print('re: ', cv_results['test_re'], '; mean:', cv_results['test_re'].mean())\n",
    "print('pr: ', cv_results['test_pr'], '; mean:', cv_results['test_pr'].mean())\n",
    "print('f1: ', cv_results['test_f1'], '; mean:', cv_results['test_f1'].mean())\n",
    "print('auc: ', cv_results['test_auc'], '; mean:', cv_results['test_auc'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YYaowWhdkTE"
   },
   "source": [
    "EAch of the metrics scores are printed above using which different classifir=ers are compared later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging classifier\n",
    "\n",
    "Bagging classifier is a ensemble method technique. Ensemble methods combines several decision trees to produce better predictive performance than utilizing a single decision tree. \n",
    "\n",
    "Objective is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees. \n",
    "\n",
    "As a result, we end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree\n",
    "\n",
    "THus, reducing variance of the decision tree.\n",
    "\n",
    "Other alternative ensemble methods are random forest, gradient bossting etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lf4PRYKeedxm"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing bagging classifier module from sklearn\n",
    "\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htFZrsekqGEG"
   },
   "outputs": [],
   "source": [
    "SelectedBC_Model=BaggingClassifier(base_estimator=Suzy_DT,n_estimators=10,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bagging classifier model called BC_MODEL\n",
    "base_estimator:The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "n_estimators :gives the number of base estimators in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAjNxiMsqUei"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=1,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelectedBC_Model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell caries out training i.e fits feature values with label values, i.e learns the mapping between x and y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-Llcl3QkgJ7"
   },
   "outputs": [],
   "source": [
    "y_pred=SelectedBC_Model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the new data input is predicted based on the features given(x_test).  It's taking in Features test( x_test) and predicting what it's corresponding y value must be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68jWcNOWkmf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7826333333333333"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SelectedBC_Model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell computes the accuracy of the classifier.\n",
    "which is 78.26.. percent. which is lesser than the model with all the features(<78.34)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1b_XZdy1kpCU"
   },
   "outputs": [],
   "source": [
    "nfolds = 10\n",
    "def tn(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): \n",
    "\treturn confusion_matrix(y_true, y_pred)[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions compute true and false , positive and negatives.And also place the scores in a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsoDELFvkrja"
   },
   "outputs": [],
   "source": [
    "scoring = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n",
    "           'fp': make_scorer(fp), 'fn': make_scorer(fn),\n",
    "           'ac' : make_scorer(accuracy_score),\n",
    "           're' : make_scorer(recall_score),\n",
    "           'pr' : make_scorer(precision_score),\n",
    "           'f1' : make_scorer(f1_score),\n",
    "           'auc' : make_scorer(roc_auc_score),\n",
    "          } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary 'scoring' is used to address each of the evaluation metrics. make_scorer creates a scorer from a performance metric or loss function. Each metric's significance is as explained before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GkI-2XakuGU"
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(SelectedBC_Model, x_train, y_train, scoring=scoring, cv=StratifiedKFold(n_splits=nfolds, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell performs cross validation using the stratified kfold cross validation method about which is explained before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yvpt9d-Bk_xS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation scores (nfolds = 10):\n",
      "tp:  [108826 108493 108854 108458 108881 108715 108875 108991 109095 109072] ; mean: 108826.0\n",
      "fn:  [51289 51622 51261 51657 51233 51399 51239 51123 51019 51042] ; mean: 51288.4\n",
      "fp:  [24755 24848 24748 24921 24983 25147 24846 24852 24864 25048] ; mean: 24901.2\n",
      "tn:  [165131 165038 165138 164965 164903 164739 165039 165033 165021 164837] ; mean: 164984.4\n",
      "ac:  [0.78273205 0.78151491 0.78283205 0.78120634 0.78224    0.78129714\n",
      " 0.78261366 0.78292795 0.78319081 0.78259938] ; mean: 0.7823154294418445\n",
      "re:  [0.67967398 0.67759423 0.67984886 0.67737564 0.68002173 0.67898497\n",
      " 0.67998426 0.68070875 0.68135828 0.68121463] ; mean: 0.6796765339116453\n",
      "pr:  [0.81468173 0.81365072 0.81476325 0.81315649 0.81337029 0.81214236\n",
      " 0.81419523 0.81431976 0.81439097 0.81324187] ; mean: 0.8137912664946277\n",
      "f1:  [0.74107921 0.73941579 0.74121689 0.73908155 0.7407425  0.7396182\n",
      " 0.74106216 0.74154383 0.74195863 0.7413963 ] ; mean: 0.7407115064660499\n",
      "auc:  [0.77465315 0.77336838 0.77475901 0.77306687 0.77422666 0.77327644\n",
      " 0.77456832 0.77491476 0.77520793 0.77465161] ; mean: 0.774269313852206\n"
     ]
    }
   ],
   "source": [
    "print('Cross Validation scores (nfolds = %d):'% nfolds)\n",
    "print('tp: ', cv_results['test_tp'], '; mean:', cv_results['test_tp'].mean())\n",
    "print('fn: ', cv_results['test_fn'], '; mean:', cv_results['test_fn'].mean())\n",
    "print('fp: ', cv_results['test_fp'], '; mean:', cv_results['test_fp'].mean())\n",
    "print('tn: ', cv_results['test_tn'], '; mean:', cv_results['test_tn'].mean())\n",
    "print('ac: ', cv_results['test_ac'], '; mean:', cv_results['test_ac'].mean())\n",
    "print('re: ', cv_results['test_re'], '; mean:', cv_results['test_re'].mean())\n",
    "print('pr: ', cv_results['test_pr'], '; mean:', cv_results['test_pr'].mean())\n",
    "print('f1: ', cv_results['test_f1'], '; mean:', cv_results['test_f1'].mean())\n",
    "print('auc: ', cv_results['test_auc'], '; mean:', cv_results['test_auc'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of the evaluation metrics for the selected bagging classifier computed above."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Suzy_Data_Selected_Attributes",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
